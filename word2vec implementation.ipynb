{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = 1.0/(1.0 + np.exp(-x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [-1, -2]])\n",
    "print sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    input_x = x\n",
    "    \n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((1, x.shape[0]))\n",
    "    \n",
    "    row_maxes = np.amax(x, axis=1).reshape((x.shape[0], 1))\n",
    "    x = np.exp(x - row_maxes)\n",
    "    row_sums = np.sum(x, axis=1).reshape((x.shape[0], 1))\n",
    "    x = x / row_sums\n",
    "    x = x.reshape(input_x.shape)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    (r, c) = x.shape\n",
    "    row_sums = np.sum(x**2, axis=1)\n",
    "    x = x / np.sqrt(row_sums.reshape((r, 1)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB = dict([(\"the\", 0), (\"quick\", 1), (\"brown\", 2), (\"fox\", 3), (\"jumped\", 4), \n",
    "              (\"over\", 5), (\"lazy\", 6), (\"dog\", 7), (\"cat\", 8), (\"OOV\", 9)])\n",
    "DATASET = type('ds', (), {})()\n",
    "V = len(VOCAB.keys())\n",
    "\n",
    "def sampleTokenIdx():\n",
    "    return random.randint(0, V-1)\n",
    "def getRandomContext(C):\n",
    "    return VOCAB.keys()[random.randint(0, V-1)], [VOCAB.keys()[random.randint(0, V-1)] for i in xrange(2 * C)]\n",
    "\n",
    "DATASET.sampleTokenIdx = sampleTokenIdx\n",
    "DATASET.getRandomContext = getRandomContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "('brown', ['fox', 'quick', 'OOV', 'cat'])\n"
     ]
    }
   ],
   "source": [
    "print DATASET.sampleTokenIdx()\n",
    "print DATASET.getRandomContext(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DxV = 2 x 10\n",
      "(10L, 2L)\n",
      "(10L, 2L)\n"
     ]
    }
   ],
   "source": [
    "D = 2\n",
    "print \"DxV =\", D, \"x\", V\n",
    "vectors = normalizeRows(np.random.randn(2 * V, D)) \n",
    "InputVectors = vectors[:V,:]   # first half\n",
    "OutputVectors = vectors[V:,:]  # second half\n",
    "print InputVectors.shape\n",
    "print OutputVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(inputVec, contextIdx):\n",
    "    N, D = OutputVectors.shape\n",
    "    \n",
    "    outputVec = OutputVectors[contextIdx]\n",
    "    \n",
    "    h = inputVec.reshape((D, 1))\n",
    "    \n",
    "    u = np.dot(OutputVectors, h)\n",
    "        \n",
    "    uT = u.reshape((1, N))\n",
    "    \n",
    "    yT= softmax(uT)\n",
    "        \n",
    "    y = yT.reshape((N,))\n",
    "                \n",
    "    cost = -np.log(y[contextIdx])\n",
    "        \n",
    "    gradPred = -1.0 * outputVec.reshape((1, D)) + np.dot(y.reshape((1, N)), OutputVectors)\n",
    "    \n",
    "    gradPred = gradPred.reshape((D,))\n",
    "\n",
    "    grad = np.dot(y.reshape((N, 1)), inputVec.reshape((1, D)))\n",
    "    \n",
    "    m = np.zeros(grad.shape)\n",
    "    \n",
    "    m[contextIdx, :] = inputVec.reshape((1, D))\n",
    "    \n",
    "    grad = grad - m    \n",
    "    \n",
    "    #assert grad.shape == outputVectors.shape    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(centerWord, contextWords):   \n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    \n",
    "    N, D = InputVectors.shape\n",
    "    \n",
    "    centerIndex = VOCAB[centerWord]\n",
    "    centerVec = InputVectors[centerIndex]\n",
    "    \n",
    "    cost = 0.0\n",
    "    gradientIn = np.zeros(InputVectors.shape)\n",
    "    gradientOut = np.zeros(OutputVectors.shape)\n",
    "\n",
    "    for contextWord in contextWords:\n",
    "        \n",
    "        print \"Context word:\", contextWord\n",
    "        \n",
    "        contextIndex = VOCAB[contextWord]\n",
    "        \n",
    "        costContext, gradInContext, gradOutContext = softmaxCostAndGradient(centerVec, contextIndex)\n",
    "        \n",
    "        cost += costContext\n",
    "        \n",
    "        gradientIn[contextIndex, :] += gradInContext\n",
    "        gradientOut += gradOutContext\n",
    "    \n",
    "    return cost, gradientIn, gradientOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = VxD (10L, 2L)\n",
      "Context word: brown\n",
      "y[contextIdx] = 0.0512490737881\n",
      "Context word: jumped\n",
      "y[contextIdx] = 0.0605259377739\n",
      "Cost = 5.77574101584\n",
      "Gradient In = [[ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [-0.4925329   0.93026938]\n",
      " [ 0.          0.        ]\n",
      " [-0.31552027  0.93180503]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "Gradient Out = [[-0.10888493  0.03850226]\n",
      " [-0.35590006  0.12584805]\n",
      " [ 0.84615913 -0.29920613]\n",
      " [-0.16625775  0.05878958]\n",
      " [ 0.82866679 -0.29302075]\n",
      " [-0.16013765  0.05662548]\n",
      " [-0.12831092  0.04537139]\n",
      " [-0.15026613  0.05313486]\n",
      " [-0.29927218  0.10582415]\n",
      " [-0.3057963   0.10813111]]\n"
     ]
    }
   ],
   "source": [
    "print \"w1 = VxD\", InputVectors.shape\n",
    "\n",
    "cost, gradientIn, gradientOut = skipgram(\"fox\", [\"brown\", \"jumped\"])\n",
    "print \"Cost =\", cost\n",
    "print \"Gradient In =\", gradientIn\n",
    "print \"Gradient Out =\", gradientOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negativeSampling(inputVec, contextIdx, K=40):\n",
    "    # Implement the cost and gradients for one input/center word vector  \n",
    "    # and one context word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample size.                                               \n",
    "    \n",
    "    N, D = OutputVectors.shape\n",
    "    \n",
    "    outputVec = OutputVectors[contextIdx]\n",
    "    \n",
    "    # Get the K random indices (rows into outputVectors)\n",
    "    k_indices = []\n",
    "    for i in xrange(K):\n",
    "        rand_index = DATASET.sampleTokenIdx()\n",
    "        k_indices.append(rand_index)        \n",
    "    \n",
    "    w_out = OutputVectors[k_indices, 0:D] # size K x D\n",
    "    \n",
    "    print \"w2 = KxD\", w_out.shape\n",
    "    \n",
    "    print \"Sampling random\", K, \"indices from output vectors as w2\"\n",
    "    \n",
    "    h = inputVec.reshape((D, 1))\n",
    "    print \"h = vT\", h.shape\n",
    "    #print h\n",
    "    \n",
    "    u = np.dot(w_out, -1.0 * h)\n",
    "    print \"u = w2 . -h\", u.shape\n",
    "    #print u\n",
    "        \n",
    "    print \"inputVec =\", inputVec, \"outpurVec =\", outputVec\n",
    "    sigmoidContext = sigmoid(np.dot(inputVec, outputVec))\n",
    "    \n",
    "    # Equation (4) (negative sampling) in “Distributed Representations of Words and Phrases and their Compositionality”\n",
    "    cost = -1.0 * np.log( sigmoidContext ) - np.sum( np.log(sigmoid(u)) )\n",
    "        \n",
    "    x1 = 1.0 - sigmoid(u) # shape is (K, 1)\n",
    "    \n",
    "    x2 = np.dot(x1.reshape((1, K)), w_out).reshape((1, D))\n",
    "    \n",
    "    gradIn = (sigmoidContext - 1.0) * outputVec.reshape((1, D)) + x2\n",
    "    \n",
    "    gradIn = gradIn.reshape(inputVec.shape)\n",
    "    \n",
    "    gradOut = np.zeros(OutputVectors.shape)\n",
    "    \n",
    "    gradOut[contextIdx, :] = inputVec * (sigmoidContext - 1.0)\n",
    "    \n",
    "    for k in k_indices:\n",
    "        gradOut[k, :] += -1.0 * inputVec * (sigmoid(np.dot(-1.0 * inputVec, OutputVectors[k])) - 1.0)\n",
    "        \n",
    "    return cost, gradIn, gradOut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
